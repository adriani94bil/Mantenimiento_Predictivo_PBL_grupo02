{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Reducción dataset\n",
    "\n",
    "El objetivo de este notebook es procesar el dataset que se ha utilizado para entrenar le modelo preliminar y tratar de reducir la dimensionalidad de este sin perder precisión. De esta forma, se identificarán que sensores no son necesarios en la máquina para identificar posibles reducciones de costes.\n",
    "\n",
    "## 5.1. Dataset y modelo inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def evaluar_precision(modelo, X, y, cv=5):\n",
    "    \"\"\"\n",
    "    Entrena un modelo usando cross-validation y retorna la precisión media.\n",
    "    :param modelo: instancia de un estimador de scikit-learn (por ej. RandomForestClassifier).\n",
    "    :param X: features (DataFrame o NumPy array).\n",
    "    :param y: variable objetivo (Series o array).\n",
    "    :param cv: número de folds para la validación cruzada.\n",
    "    :return: None (imprime la precisión media y desviación estándar).\n",
    "    \"\"\"\n",
    "    scores = cross_val_score(modelo, X, y, cv=cv, scoring='accuracy')\n",
    "    print(f\"Precisión media (CV={cv}): {scores.mean():.4f} ± {scores.std():.4f}\")\n",
    "\n",
    "\n",
    "def evaluar_metricas(modelo, X, y, cv=5):\n",
    "    \"\"\"\n",
    "    Entrena un modelo usando cross-validation y reporta Accuracy, Recall y F1.\n",
    "    \"\"\"\n",
    "    scoring = {\n",
    "        'accuracy': 'accuracy',\n",
    "        'recall': 'recall_macro',   # o 'recall_weighted' si lo prefieres\n",
    "        'f1': 'f1_macro'            # o 'f1_weighted'\n",
    "    }\n",
    "    resultados = cross_validate(modelo, X, y, cv=cv, scoring=scoring)\n",
    "    print(f\"Accuracy (CV={cv}): {resultados['test_accuracy'].mean():.4f} ± {resultados['test_accuracy'].std():.4f}\")\n",
    "    print(f\"Recall   (CV={cv}): {resultados['test_recall'].mean():.4f} ± {resultados['test_recall'].std():.4f}\")\n",
    "    print(f\"F1       (CV={cv}): {resultados['test_f1'].mean():.4f} ± {resultados['test_f1'].std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El dataset tiene incialmente un tamaño de:\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"ultimate.csv\")\n",
    "print(\"El dataset tiene incialmente un tamaño de:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, se eliminan las columnas \"Tipo\", \"Hz\" y \"medida\", ya que no son columnas que se utilizan para el entrenamiento.\n",
    "Antes de comenzar la reducción de dimensionalidad del dataset de entrenamiento, el modelo de Random Forest tiene una precisión de:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Métricas sin reducción de dimensionalidad ===\n",
      "Precisión media (CV=5): 0.8880 ± 0.0471\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X = df.drop(columns=['Tipo', 'Hz', 'medida'])  # Ejemplo, quitamos las que no van al modelo\n",
    "y = df['Tipo']\n",
    "\n",
    "modelo = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "print(\"=== Métricas sin reducción de dimensionalidad ===\")\n",
    "evaluar_precision(modelo, X, y)  # o evaluar_metricas(modelo, X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Filtrado inicial\n",
    "Reducción de dimensionalidad no supervisada\n",
    "### 5.2.1. Eliminar variables de baja varianza\n",
    "Algunas variables pueden presentar muy poca variación (casi todos los valores iguales), lo que las hace poco relevantes para la predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Métricas tras eliminar varianza muy baja ===\n",
      "Precisión media (CV=5): 0.8833 ± 0.0498\n",
      "\n",
      "Tras reducción nº1, l dataset tiene un tamaño de:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(84000, 61)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "selector_var = VarianceThreshold(threshold=0.01)\n",
    "selector_var.fit(X)\n",
    "cols_survived = X.columns[selector_var.get_support()]\n",
    "df_reduced_1 = X[cols_survived]\n",
    "\n",
    "print(\"\\n=== Métricas tras eliminar varianza muy baja ===\")\n",
    "evaluar_precision(modelo, df_reduced_1, y)\n",
    "\n",
    "print(\"\\nTras reducción nº1, l dataset tiene un tamaño de:\")\n",
    "df_reduced_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2. Análisis de correlación\n",
    "Si hay variables altamente correlacionadas entre sí (multicolinealidad), puede bastar con quedarte con una de ellas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Métricas tras eliminar correlaciones altas ===\n",
      "Precisión media (CV=5): 0.8778 ± 0.0467\n",
      "\n",
      "Tras reducción nº2, el dataset tiene un tamaño de:\n",
      "(84000, 32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Matriz de correlaciones\n",
    "corr_matrix = df_reduced_1.corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Umbral de 0.9 (ejemplo)\n",
    "to_drop = [c for c in upper_tri.columns if any(upper_tri[c] > 0.9)]\n",
    "df_reduced_2 = df_reduced_1.drop(columns=to_drop)\n",
    "\n",
    "print(\"\\n=== Métricas tras eliminar correlaciones altas ===\")\n",
    "evaluar_precision(modelo, df_reduced_2, y)\n",
    "\n",
    "print(\"\\nTras reducción nº2, el dataset tiene un tamaño de:\")\n",
    "print(df_reduced_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Métodos supervisados de selección de características\n",
    "Estos métodos consideran la relación entre cada variable y la variable objetivo (\"Tipo\") para descartar características irrelevantes o redundantes.\n",
    "\n",
    "### 5.3.1. Feature importance en modelos basados en árboles\n",
    "* Entrenar un Random Forest o un XGBoost preliminarmente y obtener la importancia de cada variable.\n",
    "* Con la importancia, filtrar aquellaspor debajo un cierto umbral o seleccionar las \"top K\" variables más relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# y_train_encoded = le.fit_transform(y_train)\n",
    "# y_test_encoded = le.transform(y_test)   # para test, usar \"transform\", NO \"fit_transform\"\n",
    "\n",
    "# model = XGBClassifier()\n",
    "# model.fit(X_train, y_train_encoded)\n",
    "\n",
    "# y_pred_encoded = model.predict(X_test)\n",
    "\n",
    "# y_pred = le.inverse_transform(y_pred_encoded) # Descodificar para obtener los nombres originales\n",
    "\n",
    "# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
    "# print(\"Matriz de Confusión:\\n\", confusion_matrix(y_test, y_pred))\n",
    "# print(\"Reporte de Clasificación:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2. Selección por modelos lineales con regularización L1 (Lasso)\n",
    "* Aplicar un modelo lineal (por ejemplo, Logistic Regression con penalización L1) que fuerce a ciertos coeficientes a ser cero y elimina variables poco relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# estimator = LogisticRegression(penalty='l1', solver='saga', max_iter=84000)\n",
    "# selector = SelectFromModel(estimator=estimator, threshold='mean')\n",
    "# selector.fit(X_train, y_train)\n",
    "\n",
    "# X_train_reduced = selector.transform(X_train)\n",
    "# X_test_reduced  = selector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Tamaño después de procesar\")\n",
    "# print(X_train_reduced.shape)\n",
    "# print(X_test_reduced.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3. Wrapper methods: Recursive Feature Elimination (RFE)\n",
    "* Consiste en entrenar un modelo y eliminar iterativamente las características "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import RFE\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# estimator = RandomForestClassifier(n_estimators=100)\n",
    "# rfe = RFE (estimator, n_features_to_select=20)\n",
    "# rfe.fit(X_train, y_train)\n",
    "\n",
    "# X_train_reduced = rfe.transform(X_train)\n",
    "# X_test_reduced = rfe.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Métodos de proyección\n",
    "En estos casos, las variables originales se combinan (mediante proyecciones lineales o no lineales) para reducir la dimensión. Algunas técnicas son:\n",
    "\n",
    "### 5.4.1. Principal component Analysis (PCA)\n",
    "* Método no supervisado que busca maximizar la varianza en los primeros componentes principales.\n",
    "* Útil cuando hay mucho colinealidad y el dataset es muy grande.\n",
    "* El inconveniente es que las nuevas variables (componentes principales) pueden perder interpretabilidad directa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Con PCA de 30 componentes, la precisión baja a 0,82. Por lo que no merece la pena aplicar esta técnica.\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# # Por ejemplo, 20 componentes\n",
    "# # 10 componentes da 0.60 de precisión\n",
    "# # 20 componentes da 0.80 de precisión\n",
    "# # 25 componentes da 0.82 de precisión\n",
    "# # 30 componentes da 0.82\n",
    "\n",
    "# pca = PCA(n_components=30, random_state=42)\n",
    "# df_reduced_3 = pca.fit_transform(df_reduced_2)\n",
    "\n",
    "# print(\"\\n=== Métricas tras PCA a 30 componentes ===\")\n",
    "# evaluar_precision(modelo, df_reduced_3, y)\n",
    "# # evaluar_metricas(modelo, df_reduced_3,y)\n",
    "\n",
    "# print(\"\\nTras reducción nº3, el dataset tiene un tamaño de:\")\n",
    "# df_reduced_3.shape\n",
    "\n",
    "print(\"Con PCA de 30 componentes, la precisión baja a 0,82. Por lo que no merece la pena aplicar esta técnica.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2. Linear Discriminant Analysis (LDA)\n",
    "* Similar al PCA pero supervisado.\n",
    "* Para problemas multiclass, LDA puede reducir la dimensionalidad hasta n_clases -1 componentes relevantes.\n",
    "* Puede ser muy efectivo si las clases están bien separadas linealmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Con LDA de 9 componentes, la precisión baja a 0.68. Por lo que no merece la pena.\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# df_reduced_2\n",
    "\n",
    "# n_clases = y.nunique()\n",
    "# n_comp_max = min(df_reduced_2.shape[1], n_clases - 1)\n",
    "# print(\"Número de clases:\", n_clases)\n",
    "# print(f\"LDA puede tener hasta {n_comp_max} componentes.\")\n",
    "\n",
    "# lda = LDA(n_components=n_comp_max)\n",
    "# df_reduced_4 = lda.fit_transform(df_reduced_2, y)\n",
    "\n",
    "# print(f\"\\n=== Métricas tras LDA con {n_comp_max} componentes ===\")\n",
    "# evaluar_precision(modelo, df_reduced_4, y)\n",
    "\n",
    "# print(\"\\nTras reducción con LDA, el dataset tiene un tamaño de:\")\n",
    "# print(df_reduced_4.shape)\n",
    "\n",
    "print(\"Con LDA de 9 componentes, la precisión baja a 0.68. Por lo que no merece la pena.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5. Comprobar precisión después de reducción de dimensionalidad\n",
    "Como conclusión, el flujo de técnicas que se han utilizado para la reducción de dimensionalidad es el siguiente:\n",
    "1. Un filtrado inicial (varianza y correlación),\n",
    "2. Método supervisado (por ejemplo, importancia de variables en Random Forest / XGBoost, o un modelo lineal con L1)\n",
    "3. PCA o LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "El dataset final tiene un tamaño de:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(84000, 32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluar_precision(modelo, df_reduced_2, y)\n",
    "# evaluar_metricas(modelo, df_reduced_2, y)\n",
    "\n",
    "print(\"\\nEl dataset final tiene un tamaño de:\")\n",
    "df_reduced_2.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas eliminadas (ordenadas alfabéticamente):\n",
      "Hz\n",
      "S1_max\n",
      "S1_var\n",
      "S2_IQR\n",
      "S2_max\n",
      "S2_min\n",
      "S2_var\n",
      "S3_IQR\n",
      "S3_max\n",
      "S3_median\n",
      "S3_min\n",
      "S4_IQR\n",
      "S4_max\n",
      "S4_min\n",
      "S4_var\n",
      "S5_IQR\n",
      "S5_median\n",
      "S5_var\n",
      "S6_IQR\n",
      "S6_max\n",
      "S6_median\n",
      "S6_min\n",
      "S6_var\n",
      "S7_IQR\n",
      "S7_max\n",
      "S7_median\n",
      "S7_min\n",
      "S7_var\n",
      "S8_IQR\n",
      "S8_mean\n",
      "S8_median\n",
      "S8_min\n",
      "S8_var\n",
      "Tipo\n",
      "medida\n"
     ]
    }
   ],
   "source": [
    "# Convertir las columnas en sets\n",
    "set_inicial = set(df.columns)\n",
    "set_reducido = set(df_reduced_2.columns)\n",
    "\n",
    "# Calcular y ordenar alfabéticamente las columnas eliminadas\n",
    "columnas_eliminadas = sorted(list(set_inicial - set_reducido))\n",
    "\n",
    "# Imprimir cada columna en una línea\n",
    "print(\"Columnas eliminadas (ordenadas alfabéticamente):\")\n",
    "for columna in columnas_eliminadas:\n",
    "    print(columna)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No hay ningún sensor para el cual se han eliminado todos los estadísticos provenientes que parten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahora df_reduced_2 tiene un tamaño de:\n",
      "(84000, 35)\n"
     ]
    }
   ],
   "source": [
    "# Añadir columnas de vuelta a df_reduced_2\n",
    "df_reduced_2[\"Hz\"] = df[\"Hz\"]\n",
    "df_reduced_2[\"medida\"] = df[\"medida\"]\n",
    "df_reduced_2[\"Tipo\"] = df[\"Tipo\"]\n",
    "\n",
    "# Comprobamos el nuevo tamaño\n",
    "print(\"Ahora df_reduced_2 tiene un tamaño de:\")\n",
    "print(df_reduced_2.shape)\n",
    "\n",
    "\n",
    "df_reduced_2.to_csv(\"df_reduced.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
