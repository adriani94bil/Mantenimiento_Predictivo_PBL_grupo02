{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Reducción dataset\n",
    "\n",
    "El objetivo de este notebook es procesar el dataset que se ha utilizado para entrenar le modelo preliminar y tratar de reducir la dimensionalidad de este sin perder precisión. De esta forma, se identificarán que sensores no son necesarios en la máquina para identificar posibles reducciones de costes.\n",
    "\n",
    "## 5.1. Dataset y modelo inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def evaluar_precision(modelo, X, y, cv=5):\n",
    "    \"\"\"\n",
    "    Entrena un modelo usando cross-validation y retorna la precisión media.\n",
    "    :param modelo: instancia de un estimador de scikit-learn (por ej. RandomForestClassifier).\n",
    "    :param X: features (DataFrame o NumPy array).\n",
    "    :param y: variable objetivo (Series o array).\n",
    "    :param cv: número de folds para la validación cruzada.\n",
    "    :return: None (imprime la precisión media y desviación estándar).\n",
    "    \"\"\"\n",
    "    scores = cross_val_score(modelo, X, y, cv=cv, scoring='accuracy')\n",
    "    print(f\"Precisión media (CV={cv}): {scores.mean():.4f} ± {scores.std():.4f}\")\n",
    "\n",
    "\n",
    "def evaluar_metricas(modelo, X, y, cv=5):\n",
    "    \"\"\"\n",
    "    Entrena un modelo usando cross-validation y reporta Accuracy, Recall y F1.\n",
    "    \"\"\"\n",
    "    scoring = {\n",
    "        'accuracy': 'accuracy',\n",
    "        'recall': 'recall_macro',   # o 'recall_weighted' si lo prefieres\n",
    "        'f1': 'f1_macro'            # o 'f1_weighted'\n",
    "    }\n",
    "    resultados = cross_validate(modelo, X, y, cv=cv, scoring=scoring)\n",
    "    print(f\"Accuracy (CV={cv}): {resultados['test_accuracy'].mean():.4f} ± {resultados['test_accuracy'].std():.4f}\")\n",
    "    print(f\"Recall   (CV={cv}): {resultados['test_recall'].mean():.4f} ± {resultados['test_recall'].std():.4f}\")\n",
    "    print(f\"F1       (CV={cv}): {resultados['test_f1'].mean():.4f} ± {resultados['test_f1'].std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El dataset tiene incialmente un tamaño de:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(84000, 67)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"processed_data_ultimate.csv\")\n",
    "print(\"El dataset tiene incialmente un tamaño de:\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de comenzar la reducción de dimensionalidad del dataset, el modelo de Random Forest tiene una precisión de:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Métricas sin reducción de dimensionalidad ===\n",
      "Precisión media (CV=5): 0.9048 ± 0.0473\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X = df.drop(columns=['Tipo', 'Hz', 'medida'])  # Ejemplo, quitamos las que no van al modelo\n",
    "y = df['Tipo']\n",
    "\n",
    "modelo = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "print(\"=== Métricas sin reducción de dimensionalidad ===\")\n",
    "evaluar_precision(modelo, X, y)  # o evaluar_metricas(modelo, X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Filtrado inicial\n",
    "Reducción de dimensionalidad no supervisada\n",
    "### 5.2.1. Eliminar variables de baja varianza\n",
    "Algunas variables pueden presentar muy poca variación (casi todos los valores iguales), lo que las hace poco relevantes para la predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Métricas tras eliminar varianza muy baja ===\n",
      "Precisión media (CV=5): 0.9011 ± 0.0516\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Umbral de varianza muy pequeño, por ejemplo 0.01\n",
    "selector_var = VarianceThreshold(threshold=0.01)\n",
    "X_var_reduced = selector_var.fit_transform(X)\n",
    "\n",
    "print(\"\\n=== Métricas tras eliminar varianza muy baja ===\")\n",
    "evaluar_precision(modelo, X_var_reduced, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2. Análisis de correlación\n",
    "Si hay variables altamente correlacionadas entre sí (multicolinealidad), puede bastar con quedarte con una de ellas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Métricas tras eliminar correlaciones altas ===\n",
      "Precisión media (CV=5): 0.8992 ± 0.0431\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# OJO: para esto necesitamos trabajar sobre DataFrame; si es un numpy array, \n",
    "# conviene primero seleccionar las columnas \"supervivientes\" de VarianceThreshold.\n",
    "columns_survived = X.columns[selector_var.get_support()]\n",
    "X_temp = X[columns_survived]\n",
    "\n",
    "# Matriz de correlaciones\n",
    "corr_matrix = X_temp.corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Umbral de 0.9, por ejemplo\n",
    "to_drop = [c for c in upper_tri.columns if any(upper_tri[c] > 0.9)]\n",
    "X_corr_reduced = X_temp.drop(columns=to_drop)\n",
    "\n",
    "print(\"\\n=== Métricas tras eliminar correlaciones altas ===\")\n",
    "evaluar_precision(modelo, X_corr_reduced, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Métodos supervisados de selección de características\n",
    "Estos métodos consideran la relación entre cada variable y la variable objetivo (\"Tipo\") para descartar características irrelevantes o redundantes.\n",
    "\n",
    "### 5.3.1. Feature importance en modelos basados en árboles\n",
    "* Entrenar un Random Forest o un XGBoost preliminarmente y obtener la importancia de cada variable.\n",
    "* Con la importancia, filtrar aquellaspor debajo un cierto umbral o seleccionar las \"top K\" variables más relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# y_train_encoded = le.fit_transform(y_train)\n",
    "# y_test_encoded = le.transform(y_test)   # para test, usar \"transform\", NO \"fit_transform\"\n",
    "\n",
    "# model = XGBClassifier()\n",
    "# model.fit(X_train, y_train_encoded)\n",
    "\n",
    "# y_pred_encoded = model.predict(X_test)\n",
    "\n",
    "# y_pred = le.inverse_transform(y_pred_encoded) # Descodificar para obtener los nombres originales\n",
    "\n",
    "# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
    "# print(\"Matriz de Confusión:\\n\", confusion_matrix(y_test, y_pred))\n",
    "# print(\"Reporte de Clasificación:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2. Selección por modelos lineales con regularización L1 (Lasso)\n",
    "* Aplicar un modelo lineal (por ejemplo, Logistic Regression con penalización L1) que fuerce a ciertos coeficientes a ser cero y elimina variables poco relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# estimator = LogisticRegression(penalty='l1', solver='saga', max_iter=84000)\n",
    "# selector = SelectFromModel(estimator=estimator, threshold='mean')\n",
    "# selector.fit(X_train, y_train)\n",
    "\n",
    "# X_train_reduced = selector.transform(X_train)\n",
    "# X_test_reduced  = selector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Tamaño después de procesar\")\n",
    "# print(X_train_reduced.shape)\n",
    "# print(X_test_reduced.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3. Wrapper methods: Recursive Feature Elimination (RFE)\n",
    "* Consiste en entrenar un modelo y eliminar iterativamente las características "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import RFE\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# estimator = RandomForestClassifier(n_estimators=100)\n",
    "# rfe = RFE (estimator, n_features_to_select=20)\n",
    "# rfe.fit(X_train, y_train)\n",
    "\n",
    "# X_train_reduced = rfe.transform(X_train)\n",
    "# X_test_reduced = rfe.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Métodos de proyección\n",
    "En estos casos, las variables originales se combinan (mediante proyecciones lineales o no lineales) para reducir la dimensión. Algunas técnicas son:\n",
    "\n",
    "### 5.4.1. Principal component Analysis (PCA)\n",
    "* Método no supervisado que busca maximizar la varianza en los primeros componentes principales.\n",
    "* Útil cuando hay mucho colinealidad y el dataset es muy grande.\n",
    "* El inconveniente es que las nuevas variables (componentes principales) pueden perder interpretabilidad directa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Métricas tras PCA a 10 componentes ===\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Por ejemplo, 10 componentes\n",
    "pca = PCA(n_components=10, random_state=42)\n",
    "X_pca = pca.fit_transform(X_corr_reduced)\n",
    "\n",
    "print(\"\\n=== Métricas tras PCA a 10 componentes ===\")\n",
    "evaluar_precision(modelo, X_pca, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2. Linear Discriminant Analysis (LDA)\n",
    "* Similar al PCA per supervisado.\n",
    "* Para problemas multiclass, LDA puede reducir la dimensionalidad hasta n_clases -1 componentes relevantes.\n",
    "* Puede ser muy efectivo si las clases están bien separadas linealmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# lda = LDA(n_components = 2) # 2 componentes\n",
    "# x_lda = lda.fit_transform(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5. Comprobar precisión después de reducción de dimensionalidad\n",
    "Como conclusión, el flujo de técnicas que se han utilizado para la reducción de dimensionalidad es el siguiente:\n",
    "1. Un filtrado inicial (varianza y correlación),\n",
    "2. Método supervisado (por ejemplo, importancia de variables en Random Forest / XGBoost, o un modelo lineal con L1)\n",
    "3. PCA o LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evaluar_precision(modelo, X_pca, y)\n",
    "evaluar_metricas(modelo, X_pca, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
